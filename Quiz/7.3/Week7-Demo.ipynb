{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa626ef-d19c-4644-8cdb-bc478b4fe197",
   "metadata": {},
   "source": [
    "# Data Visualization with PCA\n",
    "Principal Component Analysis (PCA) is a statistical technique for dimensionality reduction and feature extraction. At its core, PCA finds a new set of orthogonal axes (the principal components) onto which your data can be projected such that:\n",
    "\n",
    "**Maximal Variance**\n",
    "\n",
    "- The first principal component (PC1) captures the largest possible variance in the data.\n",
    "\n",
    "- The second principal component (PC2), orthogonal to PC1, captures the next largest variance, and so on.\n",
    "\n",
    "**Orthogonality**\n",
    "\n",
    "- All principal components are mutually perpendicular. This ensures that each component adds unique information.\n",
    "\n",
    "**Linear Transformation**\n",
    "\n",
    "PCA is a linear mapping: each principal component is a linear combination of the original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61779b41-ff5f-420a-8aa0-4082cbbcd53a",
   "metadata": {},
   "source": [
    "### Numpy generate matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04f910-3c9e-460f-b749-70899c9952ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dc5c7-3b49-443a-97da-c36226ec4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "m=4\n",
    "np.random.seed(0)\n",
    "matrix = [[0 for i in range(m)] for x in range(n)]\n",
    "print(matrix)\n",
    "for i in range(n):\n",
    "    for x in range(m):\n",
    "        matrix[i][x] = np.random.randint(1, 101)\n",
    "print(\"This matrix is: \", matrix, \"\\n\")\n",
    "\n",
    "for line in matrix:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a897b52-5e03-4aca-be60-d08f526ae16f",
   "metadata": {},
   "source": [
    "### We also can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d58949-6d09-49ad-bd79-a71675377845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Parameters\n",
    "n = 4  # number of samples (rows)\n",
    "m = 4  # number of features (columns)\n",
    "\n",
    "# Generate the matrix\n",
    "matrix = np.random.randint(1, 101, size=(n, m)).astype(float)\n",
    "\n",
    "# Print the matrix\n",
    "print(\"The matrix is:\\n\", matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d6227-15ab-4315-8489-c15a4d8bf560",
   "metadata": {},
   "source": [
    "### Data Preprocessing - StandardScaler Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074d9f6-2de9-4064-afc0-e7810ef7c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()  # Initialize the scaler\n",
    "X_scaled = scaler.fit_transform(matrix)  # Standardize the features\n",
    "\n",
    "# Step 4: Convert the scaled features back into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "# Display the scaled data\n",
    "print(\"Standardized Features:\")\n",
    "print(X_scaled_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d4d02-dd7c-4c34-8143-4670111acee5",
   "metadata": {},
   "source": [
    "### Data Preprocessing - MinMaxScaler Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3dd41-fad1-4e8f-8796-aa5064217e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_min_max_scaled = pd.DataFrame(min_max_scaler.fit_transform(matrix))\n",
    "print(\"Min-Max Scaled Data:\")\n",
    "print(X_min_max_scaled.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8b210-f3b3-4bc9-883e-9eb53ef1abf7",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Normalizer Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e173bf5-3751-4ec2-a074-cb29cb574cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "# Normalization with L1 Norm\n",
    "l1_normalizer = Normalizer(norm='l1')\n",
    "X_l1 = l1_normalizer.fit_transform(matrix)\n",
    "X_normalized_l1 = pd.DataFrame(X_l1)\n",
    "print(\"\\nL1 Normalized Data:\\n\")\n",
    "print(X_normalized_l1.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a97e0-27ba-4a95-b3c5-6f3052b1bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization with L2 Norm\n",
    "l2_normalizer = Normalizer(norm='l2')\n",
    "X_l2 = l2_normalizer.fit_transform(matrix)\n",
    "X_normalized_l2 = pd.DataFrame(X_l2)\n",
    "print(\"\\nL2 Normalized Data:\\n\")\n",
    "print(X_normalized_l2.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a01c7-48dd-4a51-9549-c9b67d108cb2",
   "metadata": {},
   "source": [
    "## 1. Examples from Slide 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d072107-3255-4b43-b41f-24e74f66c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix = np.array([\n",
    "    [64, 99, 66, 37],\n",
    "    [17, 82, 43, 66],\n",
    "    [16, 74, 79, 13],\n",
    "    [93, 67, 23, 11]\n",
    "], dtype=float)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2de623-5490-40db-ab66-cba3a9fa3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the mean\n",
    "U = []\n",
    "mean_donaminator = n\n",
    "matrix = np.array(matrix)\n",
    "summ = 0\n",
    "\n",
    "for i in range(m):\n",
    "    for x in range(n):\n",
    "        summ += matrix[x][i]\n",
    "    U.append(float(summ / mean_donaminator))\n",
    "    summ = 0\n",
    "\n",
    "print (\"The mean is : \", U, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39201aa1-d937-425c-9560-0634d0e6c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "B_matrix = [\n",
    "    [ 16.5,  18.5,  13.25,   5.25],\n",
    "    [-30.5,   1.5,  -9.75,  34.25],\n",
    "    [-31.5,  -6.5,  26.25, -18.75],\n",
    "    [ 45.5, -13.5, -29.75, -20.75]\n",
    "]\n",
    "\n",
    "# convert to NumPy array\n",
    "B_matrix = np.array(B_matrix)\n",
    "\n",
    "# print the centered matrix\n",
    "print(\"B_matrix is:\\n\", B_matrix, \"\\n\")\n",
    "\n",
    "# compute and print its transpose\n",
    "B_matrix_transpose = B_matrix.T\n",
    "print(\"Transpose of B_matrix is:\\n\", B_matrix_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3653d-74d7-470c-ab38-d1f1027db845",
   "metadata": {},
   "source": [
    "### Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87592f91-7992-4697-a1c9-f42bc5327b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix: S = 1/(n-1) * B_matrix * B_matrix_transpose\n",
    "s_donaminator = n - 1\n",
    "S_matrix = s_donaminator * np.dot(B_matrix, B_matrix_transpose)\n",
    "\n",
    "print (\"Covariance matrix:\\n\", S_matrix, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872081d0-a992-4134-96c3-a60de6e25be5",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors by numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79e86c-0b0f-4461-9736-afed651fc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting just the eigenvalues:\n",
    "eigenvalues = np.linalg.eigvals(S_matrix)\n",
    "\n",
    "# Getting eigenvectors:\n",
    "eigenvectors = np.linalg.eig(S_matrix)\n",
    "\n",
    "print (\"Eigenvalues : \\n\", eigenvalues, \"\\n\")\n",
    "print (\"Eigenvectors (column 1) : \\n\", eigenvectors[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaebeed-c152-46b4-9f76-a579dac98688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b4e0fc-3c62-4d25-ad6f-a1738a67bf0f",
   "metadata": {},
   "source": [
    "## 2. Example from Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8c698-5114-4bde-904e-fa88fd93bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bdbbdf-3d26-44ca-be1d-53470ad73b0d",
   "metadata": {},
   "source": [
    "### 2.1. Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce079a5-3ce3-4f88-a159-e7210cab5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Load and Prepare the Data\n",
    "# -------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Species labels\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(y, iris.target_names)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f4f04-886f-4cf1-87c0-e61dca6abddc",
   "metadata": {},
   "source": [
    "### 2.2 Compute the Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777f83d-5d56-4c45-b393-138ffdf73a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is sensitive to scale, so we standardize the data first\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -------------------------\n",
    "# Compute the Covariance Matrix\n",
    "# -------------------------\n",
    "# The covariance matrix shows how variables vary with respect to each other\n",
    "cov_matrix = np.cov(X_scaled, rowvar=False)\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3a74f-c7cb-46c0-9b42-f0c68dfc3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the covariance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cov_matrix, interpolation='none', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.title(\"<YourName>+Covariance Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8b1cb-5021-4f02-8e90-1fa1a0d36b1b",
   "metadata": {},
   "source": [
    "**Covariance Matrix and Its Heatmap Visualization**:\n",
    "\n",
    "A heatmap of the covariance matrix displays the pairwise relationships between standardized features. Darker colors indicate stronger positive or negative covariances. Analysis:\n",
    "\n",
    "The covariance matrix reveals how the features move together. For example, if two features have a high positive covariance, they tend to increase or decrease in tandem. In the Iris dataset, some features are correlated, indicating redundancy. This redundancy is one reason why PCA can reduce dimensionality without losing much information. \n",
    "\n",
    "Big Takeaway: Understanding the covariance structure helps us see which features share similar information. This redundancy suggests that fewer dimensions (principal components) can effectively capture most of the variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f88e6-a740-4d78-9063-cb2b84ed1102",
   "metadata": {},
   "source": [
    "### 2.3 Compute Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7eec39-d2b5-4d2f-8a97-4ffcf9274dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Compute Eigenvalues and Eigenvectors\n",
    "# -------------------------\n",
    "# These describe the directions (eigenvectors) and magnitude (eigenvalues) of variance\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"Eigenvectors:\")\n",
    "Eigenvectors = pd.DataFrame(\n",
    "    eigenvectors,\n",
    "    index=['sepal length', 'sepal width', 'petal length', 'petal width'],\n",
    "    columns=['PC1', 'PC2', 'PC3', 'PC4']\n",
    ")\n",
    "print(Eigenvectors)\n",
    "\n",
    "# Plot eigenvalues in a scree plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "components = np.arange(1, len(eigenvalues) + 1)\n",
    "plt.bar(components, eigenvalues, color='teal', alpha=0.7)\n",
    "plt.xlabel('Component Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('<YourName>+Scree Plot (Eigenvalues)')\n",
    "plt.xticks(components)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b64b1e-5938-4dd8-b36c-5fca430e69e4",
   "metadata": {},
   "source": [
    "**Eigenvalues and Scree Plot**\n",
    "\n",
    "Visualization:\n",
    "\n",
    "A scree plot displays the eigenvalues associated with each principal component.\n",
    "Both individual and cumulative explained variance ratios are plotted.\n",
    "Analysis:\n",
    "\n",
    "- The eigenvalues indicate how much variance each principal component captures.\n",
    "- The first principal component (PC1) has the highest eigenvalue, meaning it explains the most variance.\n",
    "- The cumulative plot helps determine the number of components needed to retain a desired level of total variance (often 80â€“95%).\n",
    "\n",
    "Big Takeaway:\n",
    "Most of the data's variance is captured by the first few components. This means that even though the original dataset may be high-dimensional, its essential structure can be represented in a much lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf9f73-97af-48b6-a910-df382dbe4d30",
   "metadata": {},
   "source": [
    "### 2.4 2D PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0d8d3-0e4f-4bc6-b314-bdcf9bb1ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5A. 2D PCA\n",
    "pca2d = PCA(n_components=2)\n",
    "X_pca_2d = pca2d.fit_transform(X_scaled)\n",
    "pca2d_df = pd.DataFrame(data=X_pca_2d, columns=['PC1', 'PC2'])\n",
    "pca2d_df['species'] = df['species']\n",
    "\n",
    "# Print explained variance ratio for 2D PCA\n",
    "print(\"2D PCA Explained Variance Ratio:\")\n",
    "print(pca2d.explained_variance_ratio_)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for species in iris.target_names:\n",
    "    subset = pca2d_df[pca2d_df['species'] == species]\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=species, alpha=0.7)\n",
    "    \n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('<YourName>+2D PCA of Iris Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4b542-dc84-4454-81cc-5517cac86c8f",
   "metadata": {},
   "source": [
    "**2D PCA Scatter Plot**\n",
    "\n",
    "Visualization:\n",
    "\n",
    "- A 2D scatter plot of the data projected onto PC1 and PC2.\n",
    "- Different colors indicate different Iris species.\n",
    "\n",
    "Analysis:\n",
    "\n",
    "- The plot shows clear clustering of the Iris species, indicating that the first two principal components capture the essential differences between the species.\n",
    "- Overlapping regions might suggest that some species have similar characteristics or that more subtle differences exist.\n",
    "\n",
    "Big Takeaway:\n",
    "The 2D projection reveals that the majority of the discriminative information in the Iris dataset is contained within the first two principal components, making it easier to distinguish between species visually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f44f1-2e09-4ee9-9ee4-392012fb2e76",
   "metadata": {},
   "source": [
    "### 2.5 Biplot (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6c14b-aad5-4b75-94a3-a07520562289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Visualization : Biplot (2D)\n",
    "# -------------------------\n",
    "def biplot(score, coeff, labels=None):\n",
    "    xs = score[:, 0]\n",
    "    ys = score[:, 1]\n",
    "    n = coeff.shape[0]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Scatter plot of the PCA scores\n",
    "    plt.scatter(xs, ys, c=y, cmap='viridis', alpha=0.5)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Plot arrows for feature loadings, scaled for visibility\n",
    "        plt.arrow(0, 0, coeff[i, 0] * 3, coeff[i, 1] * 3, color='r', alpha=0.7, head_width=0.1)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i, 0] * 3.2, coeff[i, 1] * 3.2, \"Var\"+str(i+1), color='r', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i, 0] * 3.2, coeff[i, 1] * 3.2, labels[i], color='r', ha='center', va='center')\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(\"<YourName>+Biplot\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the loadings for the 2D PCA (these are the eigenvectors for PC1 and PC2)\n",
    "loadings = pca2d.components_.T\n",
    "\n",
    "# Create a biplot using the 2D PCA scores and loadings\n",
    "biplot(X_pca_2d, loadings, labels=feature_names)\n",
    "\n",
    "# Explanation:\n",
    "# The biplot overlays the PCA scores with arrows representing the loadings of the original features.\n",
    "# This visualization helps you understand which features influence each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08305a-b419-4638-8aa5-f2378ae6f0d9",
   "metadata": {},
   "source": [
    "**Biplot (2D)**\n",
    "\n",
    "Visualization:\n",
    "\n",
    "- A biplot overlays the 2D PCA scatter plot with arrows representing the loadings (i.e., contributions) of the original features.\n",
    "- The direction and length of the arrows indicate how strongly each original feature influences the principal components.\n",
    "\n",
    "Analysis:\n",
    "\n",
    "- The biplot shows which features are driving the separation between species.\n",
    "- For instance, longer arrows indicate a strong influence on the principal component, and the direction suggests how the feature contributes to the variance.\n",
    "- This helps in interpreting the principal components in terms of the original features.\n",
    "\n",
    "Big Takeaway:\n",
    "- The biplot bridges the gap between the reduced-dimensional representation and the original variables. It reveals which features are most important in differentiating the species, providing a deeper understanding of the underlying data structure.\n",
    "\n",
    "Overall Big Takeaway\n",
    "\n",
    "- The combined visualizations demonstrate that:\n",
    "\n",
    "Dimensionality Reduction Works: Most of the variance in the Iris dataset is captured by the first few principal components, meaning we can reduce complexity without significant information loss.\n",
    "Natural Clusters Exist: The 2D and 3D scatter plots clearly show clusters corresponding to the Iris species, confirming that the dataset has inherent structure.\n",
    "Feature Contributions are Key: The biplot shows which original features contribute most to this structure, offering insights into the characteristics that differentiate the species.\n",
    "In summary, PCA not only simplifies the data but also reveals the critical structure and relationships within it, making it a powerful tool for both analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd99fd-45e4-48ac-848c-d9155d071a55",
   "metadata": {},
   "source": [
    "### Practice: 3D PCA Visualization\n",
    "\n",
    "In your code, please clearly label the data and include annotations for the x and y axes. \n",
    "\n",
    "Additionally, add your name to the title. \n",
    "\n",
    "Finally, refer to the observations from the 2D PCA visualization and share your thoughts on the 3D PCA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d248cf-3a40-41b1-b060-d5f7b5dd17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D PCA\n",
    "pca3d = PCA(n_components=3)\n",
    "columns=['PC1', 'PC2', 'PC3']\n",
    "\n",
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac5436-695f-4e02-9bad-559fce95f6d4",
   "metadata": {},
   "source": [
    "### 3D PCA Scatter Plot observation\n",
    "\n",
    "Your answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eead4f-c6c5-4b38-b333-1f3f2114548f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
